{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "URt4Q-TNmNDj",
        "4Qpcib1lo92R",
        "WUjNXPoVpFY8",
        "L3FGskainy8v",
        "i_-mfrlwq8_S",
        "z_jGywr11D9R",
        "7aQjNM0hDaBb",
        "lwyYAIoa2JEU",
        "xKHiyuLPEWqt",
        "Xo6RGUOIFw7B",
        "RB8w7gjRGO99",
        "YLeub0BnG8em",
        "uP7Om2rWHajm",
        "3Puljnq8ISiO",
        "2t1DZseOI9MX",
        "_yMrC0uXJjtK",
        "7P9SL3hlKcRN"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hindhammad24/Computer-Vision-Project/blob/main/computervisionproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plantify** -  Plant Health Detector\n"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By Hind Hammad and Manmeet Sagri**"
      ],
      "metadata": {
        "id": "QAc1aPxHoIVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasification**: Assigns one label to the whole image for example \"This plant is diseased.”\n"
      ],
      "metadata": {
        "id": "ZQXFOe5NoVDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "\n",
        "This project, Plantify or Plant Health Detector, applies computer vision to classify plant images as healthy or diseased.\n",
        "The goal is to help everyday gardeners identify early signs of plant disease before visible damage becomes severe.\n",
        "By fine-tuning a YOLOv8 classification model, we aim to train a system that recognizes general plant health from photos taken in outdoor garden conditions.\n",
        "\n",
        "**Problem**\n",
        "\n",
        "Many home gardeners struggle to detect early plant stress or infection, which often leads to preventable plant loss.\n",
        "Our model analyzes plant photos and classifies them into two categories, Healthy and Diseased, to support better plant care and timely action.\n",
        "\n",
        "**Users**\n",
        "\n",
        "The primary users are home gardeners and plant enthusiasts who want a quick, AI-based way to check the health of their plants.\n",
        "In practice, this could be integrated into a mobile or web app that allows users to upload or capture plant images and instantly get feedback on their health condition.\n",
        "\n",
        "**Feedback and Revisions**\n",
        "\n",
        "After receiving feedback from Dr. Yalcin and our classmates, we refined our project to make it more focused and realistic. We chose to concentrate on a smaller set of common outdoor garden plants because they are widely grown and show clear visual signs of health or disease. Our model now classifies only two conditions, healthy or diseased, regardless of plant type. This change keeps the project manageable and ensures that Plantify remains practical and useful for home and community gardeners who care for outdoor plants.\n",
        "\n",
        "\n",
        "**Issues and Biases in Data Collection**\n",
        "\n",
        "While collecting and combining data, we encountered several minor issues:\n",
        "\n",
        "- **Misplaced images:** Some images were incorrectly categorized (e.g., healthy leaves inside diseased folders). We manually reviewed and corrected these errors.\n",
        "\n",
        "- **Class imbalance:** The dataset contained more diseased images (≈63%) than healthy ones (≈37%). To reduce this bias, we oversampled healthy images during training and applied data augmentation techniques such as flipping, rotation, and color adjustments.\n",
        "\n",
        "- **Lighting and background variation**: While most Kaggle images had uniform backgrounds. To improve consistency, we ensured diverse conditions were included in both classes.\n",
        "\n",
        "We addressed these issues through careful verification, balancing, and augmentation, making the dataset more representative of real-world garden conditions."
      ],
      "metadata": {
        "id": "ZIi4bjh7mIJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect to google Drive**"
      ],
      "metadata": {
        "id": "4Qpcib1lo92R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "oo4aY5gtpudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8421dd89-4806-4e69-b32c-00798997e052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial step connects the Colab notebook to Google Drive, allowing both of us (Hind and Manmeet) to access and save files directly from our shared Drive folder.\n",
        "This ensures the dataset and training outputs are automatically saved and synchronized. This step allows Colab to read image files from Drive, run augmentation and training scripts, and then save the processed data and results back into Drive automatically.\n",
        "We confirmed that our dataset folders, Healthy and Diseased, were correctly located inside the main project directory Plantify.\n",
        "This connection ensures our files stay safe and synced, and we can easily resume work later without losing progress.."
      ],
      "metadata": {
        "id": "kKCFdkZsp3AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/plantify\"   # This folder contains: Healthy/ and Diseased/\n",
        "HEALTHY_DIR = f\"{ROOT}/Healthy\"\n",
        "DISEASED_DIR = f\"{ROOT}/Diseased\"\n",
        "\n",
        "# Quick counts\n",
        "h_files = glob.glob(f\"{HEALTHY_DIR}/*\")\n",
        "d_files = glob.glob(f\"{DISEASED_DIR}/*\")\n",
        "print(\"Healthy:\", len(h_files))\n",
        "print(\"Diseased:\", len(d_files))"
      ],
      "metadata": {
        "id": "fW4IpnQE4XVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d190f0-cc5d-4b16-8bd3-e686c3c7f4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy: 8981\n",
            "Diseased: 15306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first counted how many images were in each class folder, Healthy and Diseased, to understand the class balance before training.\n",
        "We found that the Healthy class had 8,981 images, while the Diseased class had 15,276 images, meaning the dataset was unbalanced.\n",
        "This imbalance can make the model biased toward predicting diseased plants more often, so we planned to fix it through augmentation adding more Healthy samples artificially."
      ],
      "metadata": {
        "id": "iJ1DyuBi41b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clone YOLOv8 Repository**"
      ],
      "metadata": {
        "id": "WUjNXPoVpFY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: We cloned and installed YOLOv8 from the official Ultralytics GitHub repository\n",
        "\n",
        "\n",
        "# Reset to root and recreate /content directory since the first time it gave us error\n",
        "%cd /\n",
        "!mkdir -p /content\n",
        "%cd /content\n",
        "\n",
        "# Remove any previous YOLO installation\n",
        "!rm -rf ultralytics\n",
        "\n",
        "# Clone\n",
        "!git clone https://github.com/ultralytics/ultralytics.git\n",
        "\n",
        "# Install YOLOv8 in editable mode\n",
        "!python -m pip install -q -e /content/ultralytics\n",
        "\n",
        "# Move into the YOLO folder\n",
        "%cd /content/ultralytics\n",
        "\n",
        "# Import YOLO\n",
        "import ultralytics, os, glob, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"✅ YOLO installed successfully and environment ready.\")\n"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a8faba-3359-4e9b-939e-d69a65367a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "/content\n",
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 72350, done.\u001b[K\n",
            "remote: Counting objects: 100% (597/597), done.\u001b[K\n",
            "remote: Compressing objects: 100% (328/328), done.\u001b[K\n",
            "remote: Total 72350 (delta 453), reused 281 (delta 269), pack-reused 71753 (from 3)\u001b[K\n",
            "Receiving objects: 100% (72350/72350), 38.78 MiB | 27.50 MiB/s, done.\n",
            "Resolving deltas: 100% (54228/54228), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ultralytics\n",
            "✅ YOLO installed successfully and environment ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this initial step, we installed and cloned the official YOLOv8 repository from Ultralytics. This gives us access to the latest version of the YOLO framework, which supports image classification as per toturials.\n",
        "\n",
        "By cloning directly from GitHub, we ensure that our project follows the course instructions and uses the exact implementation specified by the professor.\n",
        "Installing the repository in editable mode (-e) allows us to modify or reload YOLO files if needed during debugging or experimentation."
      ],
      "metadata": {
        "id": "0di05nWfpU0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Description**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3FGskainy8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset, titled Plantify, was created by combining images from two open-source datasets available on Kaggle:\n",
        "\n",
        "- **PlantVillage Dataset** — https://www.kaggle.com/datasets/emmarex/plantdisease\n",
        "\n",
        "- **New Plant Diseases Dataset** — https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
        "\n",
        "In addition to these sources, we each contributed 30 original images (60 total) that we captured ourselves to improve diversity, particularly for garden plants photographed outdoors in natural lighting.\n",
        "\n",
        "The combined dataset contains images of common garden plants such as apple, tomato, grape, corn, pepper, peach, and mint, categorized into two main classes:\n",
        "\n",
        "- **Healthy**\n",
        "\n",
        "- **Diseased**\n",
        "\n",
        "Each image shows a single plant leaf photographed under natural or controlled lighting conditions. The dataset was reorganized by us to focus on plant health rather than plant type.\n",
        "\n",
        "Healthy images: **8,981**\n",
        "\n",
        "Diseased images: **15,276**\n",
        "\n",
        "Total (before our additions): **24,257** images\n",
        "\n",
        "New total after adding 60 images: **24,317** images"
      ],
      "metadata": {
        "id": "0vWhx34xdrPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naming Convention**\n",
        "\n",
        "Each image filename follows a consistent structure that identifies both the plant and its condition:\n",
        "\n",
        "**plant_condition_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *apple_healthy_1.jpg*\n",
        "\n",
        "- *peach_bacterialspot_1.jpg*\n",
        "\n",
        "This naming system ensures that the model can easily recognize each class label from the folder structure, while filenames remain traceable and easy to organize."
      ],
      "metadata": {
        "id": "i_-mfrlwq8_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hind Hammad’s Contribution**\n",
        "\n",
        "Hind added 30 new images:\n",
        "\n",
        "- 8 Healthy images\n",
        "\n",
        "- 22 Diseased images\n",
        "\n",
        "**plant_condition_hind_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *basil_healthy_hind_3.jpg*\n",
        "\n",
        "- *mint_diseased_hind_5.jpg*\n",
        "\n",
        "These images were captured in outdoor settings with different lighting conditions to help the model generalize better to real-world use cases."
      ],
      "metadata": {
        "id": "bNqVsccjrNhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manmeet Sagri’s Contribution**\n",
        "\n",
        "Manmeet added a total of 30 images to the dataset:\n",
        "\n",
        "- 12 Healthy images\n",
        "\n",
        "- 18 Diseased images\n",
        "\n",
        "All her images follow the same structured naming convention with an added identifier to indicate the contributor:\n",
        "\n",
        "**plant_condition_contributor_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *tomatoes_healthy_manmeet_4.jpg*\n",
        "\n",
        "- *parsley_diseased_manmeet_1.jpg*\n",
        "\n",
        "This convention helps maintain dataset consistency and allows us to identify contributor-specific samples for tracking and quality control."
      ],
      "metadata": {
        "id": "fOnRg1jurhtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annotations**\n",
        "\n",
        "For our project, manual annotation using external tools such as Label Studio or MakeSense.ai was not required, as our task involves image classification rather than object detection.\n",
        "We defined two labels , Healthy and Diseased, and placed each image in its respective folder. This folder-based structure automatically serves as the annotation for YOLO classification training.\n",
        "Consistent file naming conventions further ensured labeling accuracy and traceability during dataset preparation.\n",
        "\n"
      ],
      "metadata": {
        "id": "N72dscBuyX_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File Structure Setup**"
      ],
      "metadata": {
        "id": "z_jGywr11D9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning and organizing all images into the two main folders, Healthy and Diseased, our next step was to format the dataset into the folder hierarchy accepted by YOLOv8 classification.\n",
        "Since YOLO reads class labels directly from folder names, we created two subfolders inside both the train and validation directories, one for each class. The final folder structure looks like this:"
      ],
      "metadata": {
        "id": "TdGzgb_Iym3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Plantify/\n",
        "├── train/\n",
        "│   ├── Healthy/\n",
        "│   │   ├── apple_healthy_1.jpg\n",
        "│   │   ├── tomato_healthy_2.jpg\n",
        "│   │   └── ...\n",
        "│   └── Diseased/\n",
        "│       ├── apple_rust_1.jpg\n",
        "│       ├── peach_bacterialspot_2.jpg\n",
        "│       └── ...\n",
        "└── val/\n",
        "    ├── Healthy/\n",
        "    │   ├── pepper_healthy_1.jpg\n",
        "    │   └── basil_healthy_2.jpg\n",
        "    └── Diseased/\n",
        "        ├── tomato_leafmold_1.jpg\n",
        "        └── grape_blackrot_2.jpg\n"
      ],
      "metadata": {
        "id": "8oQjm-A8y_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This structure allows YOLO to automatically recognize “Healthy” and “Diseased” as the two class labels during training.\n",
        "We used a 2/3 to 1/3 split, 67% of images for training and 33% for validation, following the course guideline that specifies two-thirds for training and the rest for evaluation.\n",
        "This ensures that the model learns from most of the data but is still tested on unseen samples for fair performance evaluation."
      ],
      "metadata": {
        "id": "lUA1XqkjzfZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "7aQjNM0hDaBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we performed several basic preprocessing steps to ensure data consistency and model compatibility:\n",
        "\n",
        "- Removed misplaced images: A few healthy images were found inside the diseased folder (and vice versa). We corrected these manually.\n",
        "\n",
        "- Verified dataset integrity: We checked that every image file could open successfully and that there were no missing or unreadable files.\n",
        "\n",
        "- Standardized image naming: Using PowerShell, we renamed all files with the format plant_condition_number (e.g., apple_healthy_1.jpg) to avoid issues caused by spaces or parentheses.\n",
        "\n",
        "- Folder-based labeling: Because this is a classification task, we did not need to create .txt annotation files. The folder names (“Healthy” and “Diseased”) act as labels, which YOLO automatically reads during training.\n",
        "\n",
        "These preprocessing steps ensured that our dataset was clean, uniform, and directly compatible with YOLO classification without requiring any extra label files."
      ],
      "metadata": {
        "id": "wpnNIc_x1M5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split files**"
      ],
      "metadata": {
        "id": "lwyYAIoa2JEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split dataset 2/3 train and 1/3 val per class\n",
        "# We wrote this to convert our current Plantify/{Healthy,Diseased}\n",
        "# into the exact folder structure YOLOv8 classification expects:\n",
        "# Plantify/data/Plantify_YOLO/{train,val}/{Healthy,Diseased}\n",
        "# We enforce a 2/3 : 1/3 split inside each class for fair evaluation as per instructions.\n",
        "\n",
        "import os, glob, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/plantify\"                     # current root with Healthy/ and Diseased/\n",
        "DST = \"/content/drive/MyDrive/Plantify/data/Plantify_YOLO\"   # new YOLO-ready output\n",
        "\n",
        "\n",
        "CLASSES = [\"Healthy\", \"Diseased\"]  # our two labels\n",
        "TRAIN_RATIO = 2/3                  # per requirement: 2/3 train, 1/3 validation\n",
        "RANDOM_SEED = 42                   # we fixed a seed so results are reproducible\n",
        "\n",
        "#we set a fixed seed to keep the split consistent each time.\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Create the target folders YOLO expects\n",
        "for split in [\"train\", \"val\"]:\n",
        "    for c in CLASSES:\n",
        "        Path(f\"{DST}/{split}/{c}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# list only image files we care about\n",
        "def list_images(dir_path: Path):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "    return [p for p in dir_path.glob(\"*\") if p.suffix.lower() in exts]\n",
        "\n",
        "# perform the actual split and copy\n",
        "def split_and_copy_class(src_dir: Path, class_name: str, train_ratio: float = TRAIN_RATIO):\n",
        "    # we gather all images for this class and shuffle them\n",
        "    # so the train/val sets are randomized but reproducible due to the seed.\n",
        "    files = list_images(src_dir)\n",
        "    random.shuffle(files)\n",
        "\n",
        "    n_total = len(files)\n",
        "    n_train = int(round(n_total * train_ratio))\n",
        "    train_files = files[:n_train]\n",
        "    val_files   = files[n_train:]\n",
        "\n",
        "    # we copy files instead of moving,\n",
        "    # so our original dataset remains intact as a backup.\n",
        "    tdir = Path(DST) / \"train\" / class_name\n",
        "    vdir = Path(DST) / \"val\"   / class_name\n",
        "\n",
        "    for p in train_files:\n",
        "        shutil.copy(str(p), str(tdir / p.name))\n",
        "    for p in val_files:\n",
        "        shutil.copy(str(p), str(vdir / p.name))\n",
        "\n",
        "    print(f\"✅ {class_name:8s} -> train: {len(train_files):5d} | val: {len(val_files):5d} | total: {n_total:5d}\")\n",
        "\n",
        "# Run the split for Healthy and Diseased\n",
        "for cls in CLASSES:\n",
        "    src = Path(ROOT) / cls\n",
        "    assert src.exists(), f\"Missing class folder: {src}\"\n",
        "    split_and_copy_class(src, cls, TRAIN_RATIO)\n",
        "\n",
        "# Count files in the new structure for the report screenshot --\n",
        "def count_files(folder: Path):\n",
        "    return sum(1 for p in folder.glob(\"*\") if p.is_file())\n",
        "\n",
        "train_counts = {c: count_files(Path(DST)/\"train\"/c) for c in CLASSES}\n",
        "val_counts   = {c: count_files(Path(DST)/\"val\"/c)   for c in CLASSES}\n",
        "total_train  = sum(train_counts.values())\n",
        "total_val    = sum(val_counts.values())\n",
        "ratio_train  = total_train / max(1, (total_train + total_val))\n",
        "\n",
        "print(\"\\n Summary post-split \")\n",
        "print(\"Train counts:\", train_counts)\n",
        "print(\"Val counts  :\", val_counts)\n",
        "print(f\"Overall -> train: {total_train} | val: {total_val} | train ratio ≈ {ratio_train:.2f}\")\n",
        "print(f\"\\nYOLO-ready dataset at: {DST}\")\n"
      ],
      "metadata": {
        "id": "m0nQEG5N-08O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "079b28f0-f216-4c92-e15b-dfd6c5fb9418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Healthy  -> train:  5993 | val:  2996 | total:  8989\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2422551770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Missing class folder: {src}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msplit_and_copy_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_RATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Count files in the new structure for the report screenshot --\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2422551770.py\u001b[0m in \u001b[0;36msplit_and_copy_class\u001b[0;34m(src_dir, class_name, train_ratio)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvdir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares the exact folder structure YOLOv8 classification expects\n",
        "\n",
        "- Plantify_YOLO/ train/Healthy, train/Diseased, val/Healthy,   val/Diseased\n",
        "\n",
        "with a 2/3 : 1/3 split, class-wise. we kept the original Plantify/Healthy and Plantify/Diseased as a backup in case we need to re-split or tweak ratios. Per-class split ensures class balance is preserved across train/val."
      ],
      "metadata": {
        "id": "XvnXInPl_hMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train with YOLO’s built-in augmentation** BONUS"
      ],
      "metadata": {
        "id": "RZFhTaMx_xAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify dataset paths"
      ],
      "metadata": {
        "id": "FwuL7tp_c59K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Verify dataset paths\n",
        "# We verify that our new YOLO-ready split exists so training won't break this is just becouse we had trouble in our initial run so this is us debugging in our own way\n",
        "# Directory layout expected:\n",
        "#   /.../Plantify/data/Plantify_YOLO/{train,val}/{Healthy,Diseased}\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = \" ... \"\n",
        "\n",
        "required = [\n",
        "    f\"{DATA_DIR}/train/Healthy\",\n",
        "    f\"{DATA_DIR}/train/Diseased\",\n",
        "    f\"{DATA_DIR}/val/Healthy\",\n",
        "    f\"{DATA_DIR}/val/Diseased\",\n",
        "]\n",
        "for p in required:\n",
        "    assert Path(p).exists(), f\"Missing: {p}\"\n",
        "\n",
        "print(\"✅ Dataset structure verified.\")\n"
      ],
      "metadata": {
        "id": "uSih17i_cKUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to catch path mistakes early and save time as part of our debugging since this already happened before."
      ],
      "metadata": {
        "id": "ftL-AY9DEKfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train baseline YOLOv8 classification model with built-in augmentation for **BONUS**"
      ],
      "metadata": {
        "id": "xKHiyuLPEWqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train baseline model\n",
        "# We start with the small 'n' backbone for a quick baseline.\n",
        "# We enable YOLO's built-in augmentation to improve generalization BONUS.\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "BASE_RUN_NAME = \"cls_baseline\"\n",
        "PROJECT_DIR   = \" ... \"  # where YOLO saves outputs\n",
        "\n",
        "model = YOLO(\"yolov8n-cls.pt\")  # pre-trained classifier\n",
        "\n",
        "results = model.train(\n",
        "    data=DATA_DIR,                # YOLO will infer classes from folder names\n",
        "    epochs=15,                    # short run to get a baseline\n",
        "    imgsz=224,                    # typical size for 'n' backbone\n",
        "    batch=64,\n",
        "    lr0=0.01,                     # default starting LR\n",
        "    augment=True,                 # flips/rotations/color jitter built in\n",
        "    project=PROJECT_DIR,\n",
        "    name=BASE_RUN_NAME\n",
        ")\n",
        "\n",
        "print(\"✅ Baseline training finished.\")"
      ],
      "metadata": {
        "id": "0xgRTITXEkaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wanted a fast baseline to measure initial performance before tuning."
      ],
      "metadata": {
        "id": "cZVO3vUJFtQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate baseline & collect core metrics"
      ],
      "metadata": {
        "id": "Xo6RGUOIFw7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 : Evaluate baseline model\n",
        "# We run validation to get top1/top5 accuracy and loss, and to\n",
        "# generate confusion matrices for our report.\n",
        "\n",
        "metrics = model.val(split=\"val\")  # uses same data split\n",
        "print(\"Top-1 Accuracy:\", getattr(metrics, \"top1\", \"N/A\"))\n",
        "print(\"Top-5 Accuracy:\", getattr(metrics, \"top5\", \"N/A\"))\n",
        "print(\"✅ Validation complete.\")"
      ],
      "metadata": {
        "id": "vZ1TUMklF3G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to quantify performance and generate confusion matrix images we’ll include in the report."
      ],
      "metadata": {
        "id": "W9ZeYZ1qGHzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View confusion matrix"
      ],
      "metadata": {
        "id": "RB8w7gjRGO99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Open confusion matrix\n",
        "# We load the normalized confusion matrix image that YOLO saved.\n",
        "# We'll paste this into the PDF and discuss error patterns.\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "cm_path = Path(PROJECT_DIR) / BASE_RUN_NAME / \"confusion_matrix_normalized.png\"\n",
        "if not cm_path.exists():\n",
        "\n",
        "    cm_path = Path(PROJECT_DIR) / BASE_RUN_NAME / \"confusion_matrix.png\"\n",
        "\n",
        "assert cm_path.exists(), \"Confusion matrix image not found.\" #incase not found for debugging\n",
        "display(Image.open(cm_path))\n",
        "print(\"✅ Displayed confusion matrix:\", cm_path)\n"
      ],
      "metadata": {
        "id": "Gj8ch1UIGVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our observation to include in report ..."
      ],
      "metadata": {
        "id": "ATqdoiZ6GxPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize predictions on validation images"
      ],
      "metadata": {
        "id": "YLeub0BnG8em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 : Visualize predictions\n",
        "# We generate prediction images to visually inspect correct vs. failed cases.\n",
        "# Saved images are good evidence for the report.\n",
        "\n",
        "pred_results = model.predict(\n",
        "    source=f\"{DATA_DIR}/val\",     # run on validation set\n",
        "    save=True,                    # save annotated predictions\n",
        "    imgsz=224,\n",
        "    project=PROJECT_DIR,\n",
        "    name=f\"{BASE_RUN_NAME}_preds\"\n",
        ")\n",
        "\n",
        "print(\"✅ Saved prediction images to:\", Path(PROJECT_DIR) / f\"{BASE_RUN_NAME}_preds\")"
      ],
      "metadata": {
        "id": "zcCBPIRQHAnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to understand failure modes glare, shadow, clutter, subtle discoloration, etc."
      ],
      "metadata": {
        "id": "8AuR9PaIHSYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detailed classification report"
      ],
      "metadata": {
        "id": "uP7Om2rWHajm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Per class precision sklearn\n",
        "# YOLO already reports accuracy, but we also want per-class PR/F1\n",
        "# for a clearer bias picture Healthy vs. Diseased.\n",
        "\n",
        "import glob, numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# YOLO sorts class names alphabetically by folder name\n",
        "class_names = [\"Diseased\", \"Healthy\"]\n",
        "\n",
        "# Collect a sample from validation images for quick report\n",
        "val_files, true_labels = [], []\n",
        "for i, cls in enumerate(class_names):\n",
        "    files = glob.glob(f\"{DATA_DIR}/val/{cls}/*\")\n",
        "    val_files.extend(files)\n",
        "    true_labels.extend([i]*len(files))\n",
        "\n",
        "# Sample limit size to speed up and becouse the laptop is dieing\n",
        "N = min(1000, len(val_files))\n",
        "idxs = np.random.choice(len(val_files), size=N, replace=False)\n",
        "\n",
        "pred_labels = []\n",
        "for i in idxs:\n",
        "    r = model.predict(source=val_files[i], save=False, verbose=False, imgsz=224)[0]\n",
        "    pred = int(np.argmax(r.probs.data.cpu().numpy()))\n",
        "    pred_labels.append(pred)\n",
        "\n",
        "print(classification_report([true_labels[i] for i in idxs], pred_labels, target_names=class_names, digits=4))\n",
        "print(\"✅ Generated per-class report.\")\n"
      ],
      "metadata": {
        "id": "Yy22cVVfHeT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to quantify per-class performance and show rubric-friendly metrics."
      ],
      "metadata": {
        "id": "lx64aZWoIOzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save example worked and failed images"
      ],
      "metadata": {
        "id": "3Puljnq8ISiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save examples of correct/incorrect predictions\n",
        "# We copy a few examples into 'examples/correct' and 'examples/incorrect'\n",
        "# for the report’s qualitative analysis section.\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "examples_dir = Path(PROJECT_DIR) / BASE_RUN_NAME / \"examples\"\n",
        "(examples_dir / \"correct\").mkdir(parents=True, exist_ok=True)\n",
        "(examples_dir / \"incorrect\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "saved_correct = saved_incorrect = 0\n",
        "limit = 6  # how many of each to keep\n",
        "\n",
        "for i in idxs[:500]:  # scan a subset\n",
        "    img_path = Path(val_files[i])\n",
        "    r = model.predict(source=str(img_path), save=False, verbose=False, imgsz=224)[0]\n",
        "    pred = int(np.argmax(r.probs.data.cpu().numpy()))\n",
        "    true = 0 if \"Diseased\" in str(img_path) else 1\n",
        "\n",
        "    if pred == true and saved_correct < limit:\n",
        "        shutil.copy(str(img_path), examples_dir / \"correct\" / img_path.name)\n",
        "        saved_correct += 1\n",
        "    elif pred != true and saved_incorrect < limit:\n",
        "        shutil.copy(str(img_path), examples_dir / \"incorrect\" / img_path.name)\n",
        "        saved_incorrect += 1\n",
        "\n",
        "    if saved_correct >= limit and saved_incorrect >= limit:\n",
        "        break\n",
        "\n",
        "print(f\"✅ Saved {saved_correct} correct and {saved_incorrect} incorrect examples to:\", examples_dir)\n"
      ],
      "metadata": {
        "id": "N11g_-99IYRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to include concrete visuals in the PDF and discuss likely causes lighting, texture similarity, background clutter."
      ],
      "metadata": {
        "id": "PG_eywRFI08M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter change required by rubric"
      ],
      "metadata": {
        "id": "2t1DZseOI9MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Train a tuned variant (hyperparameter change as per requirment)\n",
        "# We increase image size and add dropout + mixup/cutmix to improve\n",
        "# generalization and reduce bias toward 'Diseased'.\n",
        "\n",
        "TUNED_RUN_NAME = \"cls_hparam_change\"\n",
        "\n",
        "model_tuned = YOLO(\"yolov8n-cls.pt\")\n",
        "\n",
        "results_tuned = model_tuned.train(\n",
        "    data=DATA_DIR,\n",
        "    epochs=20,          # a bit longer\n",
        "    imgsz=256,          # larger input for more detail but took forever to process and laptop died\n",
        "    batch=64,\n",
        "    lr0=0.01,\n",
        "    dropout=0.05,       # regularization\n",
        "    mixup=0.10,         # stronger augmentation\n",
        "    cutmix=0.20,        # stronger augmentation\n",
        "    augment=True,\n",
        "    project=PROJECT_DIR,\n",
        "    name=TUNED_RUN_NAME\n",
        ")\n",
        "\n",
        "print(\"✅ Tuned training finished.\")\n"
      ],
      "metadata": {
        "id": "Ncec6cD1JDT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to test if larger inputs and stronger regularization help the model reduce false Healthy → Diseased and improve robustness."
      ],
      "metadata": {
        "id": "7HX0de44Jbf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate tuned model and compare"
      ],
      "metadata": {
        "id": "_yMrC0uXJjtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11 : Evaluate tuned model & compare with baseline\n",
        "# We compare top-1, curves, and confusion matrices across runs to\n",
        "# show the impact of our hyperparameter change.\n",
        "\n",
        "metrics_tuned = model_tuned.val(split=\"val\")\n",
        "print(\"Baseline top-1:\", getattr(metrics, \"top1\", \"N/A\"))\n",
        "print(\"Tuned    top-1:\", getattr(metrics_tuned, \"top1\", \"N/A\"))\n",
        "\n",
        "print(\"✅ Comparison ready. Grab results.png and confusion_matrix*.png from:\")\n",
        "print(\" - Baseline:\", Path(PROJECT_DIR) / BASE_RUN_NAME)\n",
        "print(\" - Tuned   :\", Path(PROJECT_DIR) / TUNED_RUN_NAME)\n"
      ],
      "metadata": {
        "id": "yI5VxUVKJpjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we saw tuned run generally improved top-1 and Healthy-class recall, with fewer false positives on Healthy images. ..."
      ],
      "metadata": {
        "id": "wXCOkzXpJ4Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Short, ready-to-paste comparison table to be added to report"
      ],
      "metadata": {
        "id": "7P9SL3hlKcRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Print a tiny comparison summary for our report\n",
        "# We print a quick, copy-pasteable summary table of top-1 accuracy.\n",
        "\n",
        "b_top1 = getattr(metrics, \"top1\", None)\n",
        "t_top1 = getattr(metrics_tuned, \"top1\", None)\n",
        "\n",
        "print(\"\\nModel Comparison (copy into report):\")\n",
        "print(\"| Run                | imgsz | dropout | mixup | cutmix | Top-1 |\")\n",
        "print(\"|--------------------|-------|---------|-------|--------|-------|\")\n",
        "print(f\"| {BASE_RUN_NAME:18s} |  224  |  0.00   | 0.00  |  0.00  | {b_top1 if b_top1 is not None else 'N/A'} |\")\n",
        "print(f\"| {TUNED_RUN_NAME:18s} |  256  |  0.05   | 0.10  |  0.20  | {t_top1 if t_top1 is not None else 'N/A'} |\")"
      ],
      "metadata": {
        "id": "4N_XrTpwKkoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this the rubric asks to change one hyperparameter and compare; this gives a clean table to paste."
      ],
      "metadata": {
        "id": "v7ghdb2jKjfa"
      }
    }
  ]
}