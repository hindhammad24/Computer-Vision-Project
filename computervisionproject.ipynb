{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "URt4Q-TNmNDj",
        "4Qpcib1lo92R",
        "WUjNXPoVpFY8",
        "L3FGskainy8v",
        "i_-mfrlwq8_S",
        "z_jGywr11D9R",
        "7aQjNM0hDaBb",
        "lwyYAIoa2JEU",
        "xKHiyuLPEWqt",
        "Xo6RGUOIFw7B",
        "RB8w7gjRGO99",
        "YLeub0BnG8em",
        "uP7Om2rWHajm",
        "3Puljnq8ISiO",
        "2t1DZseOI9MX",
        "_yMrC0uXJjtK",
        "7P9SL3hlKcRN"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hindhammad24/Computer-Vision-Project/blob/main/computervisionproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plantify** -  Plant Health Detector\n"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By Hind Hammad and Manmeet Sagri**"
      ],
      "metadata": {
        "id": "QAc1aPxHoIVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasification**: Assigns one label to the whole image for example \"This plant is diseased.”\n"
      ],
      "metadata": {
        "id": "ZQXFOe5NoVDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "\n",
        "This project, Plantify or Plant Health Detector, applies computer vision to classify plant images as healthy or diseased.\n",
        "The goal is to help everyday gardeners identify early signs of plant disease before visible damage becomes severe.\n",
        "By fine-tuning a YOLOv8 classification model, we aim to train a system that recognizes general plant health from photos taken in outdoor garden conditions.\n",
        "\n",
        "**Problem**\n",
        "\n",
        "Many home gardeners struggle to detect early plant stress or infection, which often leads to preventable plant loss.\n",
        "Our model analyzes plant photos and classifies them into two categories, Healthy and Diseased, to support better plant care and timely action.\n",
        "\n",
        "**Users**\n",
        "\n",
        "The primary users are home gardeners and plant enthusiasts who want a quick, AI-based way to check the health of their plants.\n",
        "In practice, this could be integrated into a mobile or web app that allows users to upload or capture plant images and instantly get feedback on their health condition.\n",
        "\n",
        "**Feedback and Revisions**\n",
        "\n",
        "After receiving feedback from Dr. Yalcin and our classmates, we refined our project to make it more focused and realistic. We chose to concentrate on a smaller set of common outdoor garden plants because they are widely grown and show clear visual signs of health or disease. Our model now classifies only two conditions, healthy or diseased, regardless of plant type. This change keeps the project manageable and ensures that Plantify remains practical and useful for home and community gardeners who care for outdoor plants.\n",
        "\n",
        "\n",
        "**Issues and Biases in Data Collection**\n",
        "\n",
        "While collecting and combining data, we encountered several minor issues:\n",
        "\n",
        "- **Misplaced images:** Some images were incorrectly categorized (e.g., healthy leaves inside diseased folders). We manually reviewed and corrected these errors.\n",
        "\n",
        "- **Class imbalance:** The dataset contained more diseased images (≈63%) than healthy ones (≈37%). To reduce this bias, we oversampled healthy images during training and applied data augmentation techniques such as flipping, rotation, and color adjustments.\n",
        "\n",
        "- **Lighting and background variation**: While most Kaggle images had uniform backgrounds. To improve consistency, we ensured diverse conditions were included in both classes.\n",
        "\n",
        "We addressed these issues through careful verification, balancing, and augmentation, making the dataset more representative of real-world garden conditions."
      ],
      "metadata": {
        "id": "ZIi4bjh7mIJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect to google Drive**"
      ],
      "metadata": {
        "id": "4Qpcib1lo92R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "oo4aY5gtpudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77d5558-e63c-4959-c46b-a0f37eba4c76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial step connects the Colab notebook to Google Drive, allowing both of us (Hind and Manmeet) to access and save files directly from our shared Drive folder.\n",
        "This ensures the dataset and training outputs are automatically saved and synchronized. This step allows Colab to read image files from Drive, run augmentation and training scripts, and then save the processed data and results back into Drive automatically.\n",
        "We confirmed that our dataset folders, Healthy and Diseased, were correctly located inside the main project directory Plantify.\n",
        "This connection ensures our files stay safe and synced, and we can easily resume work later without losing progress.."
      ],
      "metadata": {
        "id": "kKCFdkZsp3AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/plantify\"   # This folder contains: Healthy/ and Diseased/\n",
        "HEALTHY_DIR = f\"{ROOT}/Healthy\"\n",
        "DISEASED_DIR = f\"{ROOT}/Diseased\"\n",
        "\n",
        "# Quick counts\n",
        "h_files = glob.glob(f\"{HEALTHY_DIR}/*\")\n",
        "d_files = glob.glob(f\"{DISEASED_DIR}/*\")\n",
        "print(\"Healthy:\", len(h_files))\n",
        "print(\"Diseased:\", len(d_files))"
      ],
      "metadata": {
        "id": "fW4IpnQE4XVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d8524d-106b-42c3-b3e8-aba6922f3990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy: 8989\n",
            "Diseased: 15328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first counted how many images were in each class folder, Healthy and Diseased, to understand the class balance before training.\n",
        "We found that the Healthy class had 8,981 images, while the Diseased class had 15,276 images, meaning the dataset was unbalanced.\n",
        "This imbalance can make the model biased toward predicting diseased plants more often, so we planned to fix it through augmentation adding more Healthy samples artificially."
      ],
      "metadata": {
        "id": "iJ1DyuBi41b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clone YOLOv8 Repository**"
      ],
      "metadata": {
        "id": "WUjNXPoVpFY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: We cloned and installed YOLOv8 from the official Ultralytics GitHub repository\n",
        "\n",
        "\n",
        "# Reset to root and recreate /content directory since the first time it gave us error\n",
        "%cd /\n",
        "!mkdir -p /content\n",
        "%cd /content\n",
        "\n",
        "# Remove any previous YOLO installation\n",
        "!rm -rf ultralytics\n",
        "\n",
        "# Clone\n",
        "!git clone https://github.com/ultralytics/ultralytics.git\n",
        "\n",
        "# Install YOLOv8 in editable mode\n",
        "!python -m pip install -q -e /content/ultralytics\n",
        "\n",
        "# Move into the YOLO folder\n",
        "%cd /content/ultralytics\n",
        "\n",
        "# Import YOLO\n",
        "import ultralytics, os, glob, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"✅ YOLO installed successfully and environment ready.\")\n"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbde15d0-dd9f-4278-94cf-a167e35ed7c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "/content\n",
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 71797, done.\u001b[K\n",
            "remote: Counting objects: 100% (615/615), done.\u001b[K\n",
            "remote: Compressing objects: 100% (341/341), done.\u001b[K\n",
            "remote: Total 71797 (delta 471), reused 281 (delta 274), pack-reused 71182 (from 3)\u001b[K\n",
            "Receiving objects: 100% (71797/71797), 38.72 MiB | 25.08 MiB/s, done.\n",
            "Resolving deltas: 100% (53765/53765), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ultralytics\n",
            "✅ YOLO installed successfully and environment ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this initial step, we installed and cloned the official YOLOv8 repository from Ultralytics. This gives us access to the latest version of the YOLO framework, which supports image classification as per toturials.\n",
        "\n",
        "By cloning directly from GitHub, we ensure that our project follows the course instructions and uses the exact implementation specified by the professor.\n",
        "Installing the repository in editable mode (-e) allows us to modify or reload YOLO files if needed during debugging or experimentation."
      ],
      "metadata": {
        "id": "0di05nWfpU0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Description**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3FGskainy8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset, titled Plantify, was created by combining images from two open-source datasets available on Kaggle:\n",
        "\n",
        "- **PlantVillage Dataset** — https://www.kaggle.com/datasets/emmarex/plantdisease\n",
        "\n",
        "- **New Plant Diseases Dataset** — https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
        "\n",
        "In addition to these sources, we each contributed 30 original images (60 total) that we captured ourselves to improve diversity, particularly for garden plants photographed outdoors in natural lighting.\n",
        "\n",
        "The combined dataset contains images of common garden plants such as apple, tomato, grape, corn, pepper, peach, and mint, categorized into two main classes:\n",
        "\n",
        "- **Healthy**\n",
        "\n",
        "- **Diseased**\n",
        "\n",
        "Each image shows a single plant leaf photographed under natural or controlled lighting conditions. The dataset was reorganized by us to focus on plant health rather than plant type.\n",
        "\n",
        "Healthy images: **8,981**\n",
        "\n",
        "Diseased images: **15,276**\n",
        "\n",
        "Total (before our additions): **24,257** images\n",
        "\n",
        "New total after adding 60 images: **24,317** images"
      ],
      "metadata": {
        "id": "0vWhx34xdrPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naming Convention**\n",
        "\n",
        "Each image filename follows a consistent structure that identifies both the plant and its condition:\n",
        "\n",
        "**plant_condition_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *apple_healthy_1.jpg*\n",
        "\n",
        "- *peach_bacterialspot_1.jpg*\n",
        "\n",
        "This naming system ensures that the model can easily recognize each class label from the folder structure, while filenames remain traceable and easy to organize."
      ],
      "metadata": {
        "id": "i_-mfrlwq8_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hind Hammad’s Contribution**\n",
        "\n",
        "Hind added 30 new images:\n",
        "\n",
        "- 8 Healthy images\n",
        "\n",
        "- 22 Diseased images\n",
        "\n",
        "**plant_condition_hind_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *basil_healthy_hind_3.jpg*\n",
        "\n",
        "- *mint_diseased_hind_5.jpg*\n",
        "\n",
        "These images were captured in outdoor settings with different lighting conditions to help the model generalize better to real-world use cases."
      ],
      "metadata": {
        "id": "bNqVsccjrNhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manmeet Sagri’s Contribution**\n",
        "\n",
        "Manmeet added a total of 30 images to the dataset:\n",
        "\n",
        "- 12 Healthy images\n",
        "\n",
        "- 18 Diseased images\n",
        "\n",
        "All her images follow the same structured naming convention with an added identifier to indicate the contributor:\n",
        "\n",
        "**plant_condition_contributor_number.jpg**\n",
        "\n",
        "\n",
        "Examples:\n",
        "\n",
        "- *tomatoes_healthy_manmeet_4.jpg*\n",
        "\n",
        "- *parsley_diseased_manmeet_1.jpg*\n",
        "\n",
        "This convention helps maintain dataset consistency and allows us to identify contributor-specific samples for tracking and quality control."
      ],
      "metadata": {
        "id": "fOnRg1jurhtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annotations**\n",
        "\n",
        "For our project, manual annotation using external tools such as Label Studio or MakeSense.ai was not required, as our task involves image classification rather than object detection.\n",
        "We defined two labels , Healthy and Diseased, and placed each image in its respective folder. This folder-based structure automatically serves as the annotation for YOLO classification training.\n",
        "Consistent file naming conventions further ensured labeling accuracy and traceability during dataset preparation.\n",
        "\n"
      ],
      "metadata": {
        "id": "N72dscBuyX_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File Structure Setup**"
      ],
      "metadata": {
        "id": "z_jGywr11D9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning and organizing all images into the two main folders, Healthy and Diseased, our next step was to format the dataset into the folder hierarchy accepted by YOLOv8 classification.\n",
        "Since YOLO reads class labels directly from folder names, we created two subfolders inside both the train and validation directories, one for each class. The final folder structure looks like this:"
      ],
      "metadata": {
        "id": "TdGzgb_Iym3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLEASE DON'T TRY RUNNING THIS CODE IT IS JUST TO SHOW OUR FILE STRUCTURE**"
      ],
      "metadata": {
        "id": "cQyuf9uZYxZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Plantify/\n",
        "├── train/\n",
        "│   ├── Healthy/\n",
        "│   │   ├── apple_healthy_1.jpg\n",
        "│   │   ├── tomato_healthy_2.jpg\n",
        "│   │   └── ...\n",
        "│   └── Diseased/\n",
        "│       ├── apple_rust_1.jpg\n",
        "│       ├── peach_bacterialspot_2.jpg\n",
        "│       └── ...\n",
        "└── val/\n",
        "    ├── Healthy/\n",
        "    │   ├── pepper_healthy_1.jpg\n",
        "    │   └── basil_healthy_2.jpg\n",
        "    └── Diseased/\n",
        "        ├── tomato_leafmold_1.jpg\n",
        "        └── grape_blackrot_2.jpg\n"
      ],
      "metadata": {
        "id": "8oQjm-A8y_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This structure allows YOLO to automatically recognize “Healthy” and “Diseased” as the two class labels during training.\n",
        "We used a 2/3 to 1/3 split, 67% of images for training and 33% for validation, following the course guideline that specifies two-thirds for training and the rest for evaluation.\n",
        "This ensures that the model learns from most of the data but is still tested on unseen samples for fair performance evaluation."
      ],
      "metadata": {
        "id": "lUA1XqkjzfZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "7aQjNM0hDaBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we performed several basic preprocessing steps to ensure data consistency and model compatibility:\n",
        "\n",
        "- Removed misplaced images: A few healthy images were found inside the diseased folder (and vice versa). We corrected these manually.\n",
        "\n",
        "- Verified dataset integrity: We checked that every image file could open successfully and that there were no missing or unreadable files.\n",
        "\n",
        "- Standardized image naming: Using PowerShell, we renamed all files with the format plant_condition_number (e.g., apple_healthy_1.jpg) to avoid issues caused by spaces or parentheses.\n",
        "\n",
        "- Folder-based labeling: Because this is a classification task, we did not need to create .txt annotation files. The folder names (“Healthy” and “Diseased”) act as labels, which YOLO automatically reads during training.\n",
        "\n",
        "These preprocessing steps ensured that our dataset was clean, uniform, and directly compatible with YOLO classification without requiring any extra label files."
      ],
      "metadata": {
        "id": "wpnNIc_x1M5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split files**"
      ],
      "metadata": {
        "id": "lwyYAIoa2JEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fast 2/3 split for YOLO dataset\n",
        "# using SYMLINKS instead of copying\n",
        "\n",
        "# In our project, we first tried copying all images from our Drive folders,\n",
        "# but it took hours because Google Drive is very slow with large datasets.\n",
        "# So we switched to creating SYMLINKS or shortcuts instead of\n",
        "# physically copying files. This keeps the same folder structure but\n",
        "# runs much faster, finishing in less than a minute even for 20k+ images.\n",
        "\n",
        "from pathlib import Path\n",
        "import os, random, shutil\n",
        "\n",
        "# Our dataset location\n",
        "# We kept our original Plantify dataset inside Google Drive\n",
        "ROOT = Path(\"/content/drive/MyDrive/plantify\")\n",
        "CLASSES = [\"Healthy\", \"Diseased\"]  # class names\n",
        "\n",
        "# Instead of putting it back on Drive\n",
        "# we save it locally in Colab's /content directory.\n",
        "DST = Path(\"/content/Plantify_YOLO\")\n",
        "\n",
        "# Before creating new links, we clear any old split to start fresh since we initially had trouble with this\n",
        "if DST.exists():\n",
        "    shutil.rmtree(DST)\n",
        "\n",
        "# Ratio and random seed setup\n",
        "# We chose to use a 2/3 : 1/3 train validation split as per the instructions\n",
        "TRAIN_RATIO = 2/3\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# YOLO expects a {train,val}/{Healthy,Diseased}\n",
        "for split in [\"train\", \"val\"]:\n",
        "    for c in CLASSES:\n",
        "        (DST / split / c).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to list all image files in each class folder\n",
        "# This helps us grab only actual image files and ignore others just incase there is other files\n",
        "def list_images(dir_path: Path):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "    return [p for p in dir_path.iterdir() if p.is_file() and p.suffix.lower() in exts]\n",
        "\n",
        "# Function to create a symlink\n",
        "# A symlink points to the original file, so no data is duplicated.\n",
        "# If the link already exists, we skip it to save time.\n",
        "def safe_symlink(src: Path, dst: Path) -> bool:\n",
        "    if dst.exists() or dst.is_symlink():\n",
        "        return False\n",
        "    os.symlink(src.resolve(), dst)\n",
        "    return True\n",
        "\n",
        "# Function to split and link files for each class\n",
        "# Here, we shuffle images so training is randomized, split them by ratio,\n",
        "# and then create symlinks to the original files in the correct train/val folders.\n",
        "def split_and_link_class(src_dir: Path, class_name: str, train_ratio: float = TRAIN_RATIO):\n",
        "    files = list_images(src_dir)\n",
        "    random.shuffle(files)\n",
        "    n_total = len(files)\n",
        "    n_train = int(round(n_total * train_ratio))\n",
        "\n",
        "    train_files = files[:n_train]\n",
        "    val_files   = files[n_train:]\n",
        "\n",
        "    tdir = DST / \"train\" / class_name\n",
        "    vdir = DST / \"val\"   / class_name\n",
        "\n",
        "    made_train = sum(safe_symlink(p, tdir / p.name) for p in train_files)\n",
        "    made_val   = sum(safe_symlink(p, vdir / p.name) for p in val_files)\n",
        "\n",
        "    print(f\"{class_name:8s} -> train: {made_train:5d} | val: {made_val:5d} | total: {n_total:5d}\")\n",
        "\n",
        "# Run the split for both classes\n",
        "for cls in CLASSES:\n",
        "    src = ROOT / cls\n",
        "    if not src.exists():\n",
        "        raise FileNotFoundError(f\"Missing class folder: {src}\")\n",
        "    split_and_link_class(src, cls)\n",
        "\n",
        "print(\" Symlink split complete at:\", DST)\n",
        "\n",
        "# Quick check of the final counts\n",
        "# This gives us a summary of how many files were linked into train vs val.\n",
        "def count_files(folder: Path) -> int:\n",
        "    return sum(1 for p in folder.iterdir() if p.is_file())\n",
        "\n",
        "train_counts = {c: count_files(DST / \"train\" / c) for c in CLASSES}\n",
        "val_counts   = {c: count_files(DST / \"val\"   / c) for c in CLASSES}\n",
        "total_train  = sum(train_counts.values())\n",
        "total_val    = sum(val_counts.values())\n",
        "ratio_train  = total_train / max(1, (total_train + total_val))\n",
        "\n",
        "\n",
        "print(\"Train counts:\", train_counts)\n",
        "print(\"Val counts  :\", val_counts)\n",
        "print(f\"YOLO-ready dataset at: {DST}\")\n"
      ],
      "metadata": {
        "id": "m0nQEG5N-08O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b3e57b-b57f-4aa5-b5d7-ad5834d00d85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy  -> train:  5993 | val:  2996 | total:  8989\n",
            "Diseased -> train: 10219 | val:  5109 | total: 15328\n",
            " Symlink split complete at: /content/Plantify_YOLO\n",
            "Train counts: {'Healthy': 5993, 'Diseased': 10219}\n",
            "Val counts  : {'Healthy': 2996, 'Diseased': 5109}\n",
            "YOLO-ready dataset at: /content/Plantify_YOLO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation for what we did and why.**\n",
        "\n",
        "At first, we used the old method of copying images from Google Drive into new train and validation folders using shutil.copy. However, this took a very long time because our dataset has over 20,000 images, and Drive is slow when handling many small files. Our first run took more than 53 minutes and we asked chatgpt how long we expect it to take it said more than 1 to 3+ hours.\n",
        "\n",
        "After researching how to make this process faster, we discovered that symlinks symbolic links can be used to create lightweight pointers to existing files instead of duplicating them. This allowed us to organize our dataset into the YOLO format instantly without copying or moving any images.\n",
        "\n",
        "We asked ChatGPT for help to guide us through this process, debug errors, and adapt the method to work inside Google Colab. During our research, we also found useful explanations and discussions from:\n",
        "\n",
        "- GitHub issue on Jupytext and symlinks - https://github.com/mwouts/jupytext/issues/696\n",
        "\n",
        "\n",
        "- Unix StackExchange post about what symlinking is and how it works - https://unix.stackexchange.com/questions/252561/what-is-a-symbolic-link-made-from-understanding-the-structure-of-symlinks\n",
        "\n",
        "\n",
        "This new method made our workflow much easier, faster, and more efficient. We learned the difference between copying and linking files, and now our dataset setup runs in seconds instead of hours, while still following YOLO's required structure."
      ],
      "metadata": {
        "id": "XvnXInPl_hMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train with YOLO’s built-in augmentation** BONUS"
      ],
      "metadata": {
        "id": "RZFhTaMx_xAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify dataset paths"
      ],
      "metadata": {
        "id": "FwuL7tp_c59K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Verify dataset paths\n",
        "# We verify that our new YOLO-ready split exists so training won't break this is just becouse we had trouble in our initial run so this is us debugging in our own way\n",
        "# Directory layout expected:\n",
        "#   /.../Plantify/data/Plantify_YOLO/{train,val}/{Healthy,Diseased}\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = \" ... \"\n",
        "\n",
        "required = [\n",
        "    f\"{DATA_DIR}/train/Healthy\",\n",
        "    f\"{DATA_DIR}/train/Diseased\",\n",
        "    f\"{DATA_DIR}/val/Healthy\",\n",
        "    f\"{DATA_DIR}/val/Diseased\",\n",
        "]\n",
        "for p in required:\n",
        "    assert Path(p).exists(), f\"Missing: {p}\"\n",
        "\n",
        "print(\"✅ Dataset structure verified.\")\n"
      ],
      "metadata": {
        "id": "uSih17i_cKUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to catch path mistakes early and save time as part of our debugging since this already happened before."
      ],
      "metadata": {
        "id": "ftL-AY9DEKfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train baseline YOLOv8 classification model with built-in augmentation for **BONUS**"
      ],
      "metadata": {
        "id": "xKHiyuLPEWqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train baseline model\n",
        "# We start with the small 'n' backbone for a quick baseline.\n",
        "# We enable YOLO's built-in augmentation to improve generalization BONUS.\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "BASE_RUN_NAME = \"cls_baseline\"\n",
        "PROJECT_DIR   = \" ... \"  # where YOLO saves outputs\n",
        "\n",
        "model = YOLO(\"yolov8n-cls.pt\")  # pre-trained classifier\n",
        "\n",
        "results = model.train(\n",
        "    data=DATA_DIR,                # YOLO will infer classes from folder names\n",
        "    epochs=15,                    # short run to get a baseline\n",
        "    imgsz=224,                    # typical size for 'n' backbone\n",
        "    batch=64,\n",
        "    lr0=0.01,                     # default starting LR\n",
        "    augment=True,                 # flips/rotations/color jitter built in\n",
        "    project=PROJECT_DIR,\n",
        "    name=BASE_RUN_NAME\n",
        ")\n",
        "\n",
        "print(\"✅ Baseline training finished.\")"
      ],
      "metadata": {
        "id": "0xgRTITXEkaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wanted a fast baseline to measure initial performance before tuning."
      ],
      "metadata": {
        "id": "cZVO3vUJFtQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate baseline & collect core metrics"
      ],
      "metadata": {
        "id": "Xo6RGUOIFw7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 : Evaluate baseline model\n",
        "# We run validation to get top1/top5 accuracy and loss, and to\n",
        "# generate confusion matrices for our report.\n",
        "\n",
        "metrics = model.val(split=\"val\")  # uses same data split\n",
        "print(\"Top-1 Accuracy:\", getattr(metrics, \"top1\", \"N/A\"))\n",
        "print(\"Top-5 Accuracy:\", getattr(metrics, \"top5\", \"N/A\"))\n",
        "print(\"✅ Validation complete.\")"
      ],
      "metadata": {
        "id": "vZ1TUMklF3G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to quantify performance and generate confusion matrix images we’ll include in the report."
      ],
      "metadata": {
        "id": "W9ZeYZ1qGHzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View confusion matrix"
      ],
      "metadata": {
        "id": "RB8w7gjRGO99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Open confusion matrix\n",
        "# We load the normalized confusion matrix image that YOLO saved.\n",
        "# We'll paste this into the PDF and discuss error patterns.\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "cm_path = Path(PROJECT_DIR) / BASE_RUN_NAME / \"confusion_matrix_normalized.png\"\n",
        "if not cm_path.exists():\n",
        "\n",
        "    cm_path = Path(PROJECT_DIR) / BASE_RUN_NAME / \"confusion_matrix.png\"\n",
        "\n",
        "assert cm_path.exists(), \"Confusion matrix image not found.\" #incase not found for debugging\n",
        "display(Image.open(cm_path))\n",
        "print(\"✅ Displayed confusion matrix:\", cm_path)\n"
      ],
      "metadata": {
        "id": "Gj8ch1UIGVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our observation to include in report ..."
      ],
      "metadata": {
        "id": "ATqdoiZ6GxPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize predictions on validation images"
      ],
      "metadata": {
        "id": "YLeub0BnG8em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 : Visualize predictions\n",
        "# We generate prediction images to visually inspect correct vs. failed cases.\n",
        "# Saved images are good evidence for the report.\n",
        "\n",
        "pred_results = model.predict(\n",
        "    source=f\"{DATA_DIR}/val\",     # run on validation set\n",
        "    save=True,                    # save annotated predictions\n",
        "    imgsz=224,\n",
        "    project=PROJECT_DIR,\n",
        "    name=f\"{BASE_RUN_NAME}_preds\"\n",
        ")\n",
        "\n",
        "print(\"✅ Saved prediction images to:\", Path(PROJECT_DIR) / f\"{BASE_RUN_NAME}_preds\")"
      ],
      "metadata": {
        "id": "zcCBPIRQHAnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to understand failure modes glare, shadow, clutter, subtle discoloration, etc."
      ],
      "metadata": {
        "id": "8AuR9PaIHSYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detailed classification report"
      ],
      "metadata": {
        "id": "uP7Om2rWHajm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Per class precision sklearn\n",
        "# YOLO already reports accuracy, but we also want per-class PR/F1\n",
        "# for a clearer bias picture Healthy vs. Diseased.\n",
        "\n",
        "import glob, numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# YOLO sorts class names alphabetically by folder name\n",
        "class_names = [\"Diseased\", \"Healthy\"]\n",
        "\n",
        "# Collect a sample from validation images for quick report\n",
        "val_files, true_labels = [], []\n",
        "for i, cls in enumerate(class_names):\n",
        "    files = glob.glob(f\"{DATA_DIR}/val/{cls}/*\")\n",
        "    val_files.extend(files)\n",
        "    true_labels.extend([i]*len(files))\n",
        "\n",
        "# Sample limit size to speed up and becouse the laptop is dieing\n",
        "N = min(1000, len(val_files))\n",
        "idxs = np.random.choice(len(val_files), size=N, replace=False)\n",
        "\n",
        "pred_labels = []\n",
        "for i in idxs:\n",
        "    r = model.predict(source=val_files[i], save=False, verbose=False, imgsz=224)[0]\n",
        "    pred = int(np.argmax(r.probs.data.cpu().numpy()))\n",
        "    pred_labels.append(pred)\n",
        "\n",
        "print(classification_report([true_labels[i] for i in idxs], pred_labels, target_names=class_names, digits=4))\n",
        "print(\"✅ Generated per-class report.\")\n"
      ],
      "metadata": {
        "id": "Yy22cVVfHeT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to quantify per-class performance and show rubric-friendly metrics."
      ],
      "metadata": {
        "id": "lx64aZWoIOzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save example worked and failed images"
      ],
      "metadata": {
        "id": "3Puljnq8ISiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save examples of correct/incorrect predictions\n",
        "# We copy a few examples into 'examples/correct' and 'examples/incorrect'\n",
        "# for the report’s qualitative analysis section.\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "examples_dir = Path(PROJECT_DIR) / BASE_RUN_NAME / \"examples\"\n",
        "(examples_dir / \"correct\").mkdir(parents=True, exist_ok=True)\n",
        "(examples_dir / \"incorrect\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "saved_correct = saved_incorrect = 0\n",
        "limit = 6  # how many of each to keep\n",
        "\n",
        "for i in idxs[:500]:  # scan a subset\n",
        "    img_path = Path(val_files[i])\n",
        "    r = model.predict(source=str(img_path), save=False, verbose=False, imgsz=224)[0]\n",
        "    pred = int(np.argmax(r.probs.data.cpu().numpy()))\n",
        "    true = 0 if \"Diseased\" in str(img_path) else 1\n",
        "\n",
        "    if pred == true and saved_correct < limit:\n",
        "        shutil.copy(str(img_path), examples_dir / \"correct\" / img_path.name)\n",
        "        saved_correct += 1\n",
        "    elif pred != true and saved_incorrect < limit:\n",
        "        shutil.copy(str(img_path), examples_dir / \"incorrect\" / img_path.name)\n",
        "        saved_incorrect += 1\n",
        "\n",
        "    if saved_correct >= limit and saved_incorrect >= limit:\n",
        "        break\n",
        "\n",
        "print(f\"✅ Saved {saved_correct} correct and {saved_incorrect} incorrect examples to:\", examples_dir)\n"
      ],
      "metadata": {
        "id": "N11g_-99IYRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to include concrete visuals in the PDF and discuss likely causes lighting, texture similarity, background clutter."
      ],
      "metadata": {
        "id": "PG_eywRFI08M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter change required by rubric"
      ],
      "metadata": {
        "id": "2t1DZseOI9MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Train a tuned variant (hyperparameter change as per requirment)\n",
        "# We increase image size and add dropout + mixup/cutmix to improve\n",
        "# generalization and reduce bias toward 'Diseased'.\n",
        "\n",
        "TUNED_RUN_NAME = \"cls_hparam_change\"\n",
        "\n",
        "model_tuned = YOLO(\"yolov8n-cls.pt\")\n",
        "\n",
        "results_tuned = model_tuned.train(\n",
        "    data=DATA_DIR,\n",
        "    epochs=20,          # a bit longer\n",
        "    imgsz=256,          # larger input for more detail but took forever to process and laptop died\n",
        "    batch=64,\n",
        "    lr0=0.01,\n",
        "    dropout=0.05,       # regularization\n",
        "    mixup=0.10,         # stronger augmentation\n",
        "    cutmix=0.20,        # stronger augmentation\n",
        "    augment=True,\n",
        "    project=PROJECT_DIR,\n",
        "    name=TUNED_RUN_NAME\n",
        ")\n",
        "\n",
        "print(\"✅ Tuned training finished.\")\n"
      ],
      "metadata": {
        "id": "Ncec6cD1JDT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this to test if larger inputs and stronger regularization help the model reduce false Healthy → Diseased and improve robustness."
      ],
      "metadata": {
        "id": "7HX0de44Jbf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate tuned model and compare"
      ],
      "metadata": {
        "id": "_yMrC0uXJjtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11 : Evaluate tuned model & compare with baseline\n",
        "# We compare top-1, curves, and confusion matrices across runs to\n",
        "# show the impact of our hyperparameter change.\n",
        "\n",
        "metrics_tuned = model_tuned.val(split=\"val\")\n",
        "print(\"Baseline top-1:\", getattr(metrics, \"top1\", \"N/A\"))\n",
        "print(\"Tuned    top-1:\", getattr(metrics_tuned, \"top1\", \"N/A\"))\n",
        "\n",
        "print(\"✅ Comparison ready. Grab results.png and confusion_matrix*.png from:\")\n",
        "print(\" - Baseline:\", Path(PROJECT_DIR) / BASE_RUN_NAME)\n",
        "print(\" - Tuned   :\", Path(PROJECT_DIR) / TUNED_RUN_NAME)\n"
      ],
      "metadata": {
        "id": "yI5VxUVKJpjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we saw tuned run generally improved top-1 and Healthy-class recall, with fewer false positives on Healthy images. ..."
      ],
      "metadata": {
        "id": "wXCOkzXpJ4Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Short, ready-to-paste comparison table to be added to report"
      ],
      "metadata": {
        "id": "7P9SL3hlKcRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Print a tiny comparison summary for our report\n",
        "# We print a quick, copy-pasteable summary table of top-1 accuracy.\n",
        "\n",
        "b_top1 = getattr(metrics, \"top1\", None)\n",
        "t_top1 = getattr(metrics_tuned, \"top1\", None)\n",
        "\n",
        "print(\"\\nModel Comparison (copy into report):\")\n",
        "print(\"| Run                | imgsz | dropout | mixup | cutmix | Top-1 |\")\n",
        "print(\"|--------------------|-------|---------|-------|--------|-------|\")\n",
        "print(f\"| {BASE_RUN_NAME:18s} |  224  |  0.00   | 0.00  |  0.00  | {b_top1 if b_top1 is not None else 'N/A'} |\")\n",
        "print(f\"| {TUNED_RUN_NAME:18s} |  256  |  0.05   | 0.10  |  0.20  | {t_top1 if t_top1 is not None else 'N/A'} |\")"
      ],
      "metadata": {
        "id": "4N_XrTpwKkoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we did this the rubric asks to change one hyperparameter and compare; this gives a clean table to paste."
      ],
      "metadata": {
        "id": "v7ghdb2jKjfa"
      }
    }
  ]
}